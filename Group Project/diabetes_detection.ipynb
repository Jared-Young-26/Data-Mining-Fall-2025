{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b86f438",
   "metadata": {},
   "source": [
    "# Diabetes Detection Concept\n",
    "This notebook is not functional and by no means efficient but illustrates a processing pipeline for predicting classes based on association rules and weighted contributions to the parent class in which it belongs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2650375",
   "metadata": {},
   "source": [
    "## Download Dataset for \"Diabetes Health Indicators\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b214ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"mohankrishnathalla/diabetes-health-indicators-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ada1aac",
   "metadata": {},
   "source": [
    "## Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c332f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def preprocess_data(path):\n",
    "\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(f\"{path}/diabetes_dataset.csv\")\n",
    "\n",
    "    # Age (18–90)\n",
    "    data['age_group'] = pd.cut(data['age'], \n",
    "                            bins=[18, 30, 40, 50, 60, 70, 80, 90],\n",
    "                            labels=['18-29', '30-39', '40-49', '50-59', '60-69', '70-79', '80-90'])\n",
    "\n",
    "    # Alcohol consumption per week\n",
    "    data['alcohol_group'] = pd.cut(data['alcohol_consumption_per_week'],\n",
    "                                bins=[0, 1, 3, 7, 14, 21, 100],\n",
    "                                labels=['None', 'Low', 'Moderate', 'Frequent', 'Heavy', 'Extreme'])\n",
    "\n",
    "    # Physical activity per week\n",
    "    data['activity_group'] = pd.cut(data['physical_activity_minutes_per_week'],\n",
    "                                    bins=[0, 60, 120, 180, 300, 600, 1000],\n",
    "                                    labels=['Sedentary', 'Low', 'Moderate', 'Active', 'VeryActive', 'Athlete'])\n",
    "\n",
    "    # Diet score\n",
    "    data['diet_group'] = pd.cut(data['diet_score'], \n",
    "                                bins=[0, 3, 5, 7, 8.5, 10],\n",
    "                                labels=['Poor', 'Fair', 'Good', 'VeryGood', 'Excellent'])\n",
    "\n",
    "    # Sleep (hours/day)\n",
    "    data['sleep_group'] = pd.cut(data['sleep_hours_per_day'], \n",
    "                                bins=[3, 6.3, 7, 7.7, 10],\n",
    "                                labels=['Deprivation', 'Poor', 'Healthy', 'Oversleeping'])\n",
    "\n",
    "    # BMI\n",
    "    data['bmi_group'] = pd.cut(data['bmi'], \n",
    "                                bins=[15, 23.2, 25.6, 28, 39.2],\n",
    "                                labels=['Underweight', 'Healthy', 'Overweight', 'Obesity'])\n",
    "\n",
    "    # Waist-to-Hip Ratio\n",
    "    data['waist_ratio'] = pd.cut(data['waist_to_hip_ratio'],\n",
    "                                bins=[0.67, 0.82, 0.86, 0.89, 1.06],\n",
    "                                labels=['Low', 'Moderate', 'High', 'VeryHigh'])\n",
    "\n",
    "\n",
    "    # --- Cardiovascular Indicators ---\n",
    "\n",
    "    # Systolic Blood Pressure (mmHg)\n",
    "    data['systolic_bp'] = pd.cut(data['systolic_bp'], \n",
    "                                bins=[80, 100, 120, 130, 140, 160, 200],\n",
    "                                labels=['Low', 'Ideal', 'Elevated', 'Hypertension_Stage1', 'Hypertension_Stage2', 'Crisis'])\n",
    "\n",
    "    # Diastolic Blood Pressure (mmHg)\n",
    "    data['diastolic_bp'] = pd.cut(data['diastolic_bp'], \n",
    "                                bins=[40, 60, 80, 90, 100, 120],\n",
    "                                labels=['Low', 'Ideal', 'Elevated', 'Stage1', 'Stage2'])\n",
    "\n",
    "    # Heart Rate (bpm)\n",
    "    data['heart_rate'] = pd.cut(data['heart_rate'],\n",
    "                                bins=[40, 60, 80, 100, 120, 200],\n",
    "                                labels=['Bradycardia', 'Normal', 'Elevated', 'Tachycardia', 'Extreme'])\n",
    "\n",
    "    # --- Lipid Profile ---\n",
    "\n",
    "    # Total Cholesterol (mg/dL)\n",
    "    data['cholesterol_level'] = pd.cut(data['cholesterol_total'],\n",
    "                                    bins=[100, 160, 200, 240, 300, 400],\n",
    "                                    labels=['Low', 'Desirable', 'Borderline', 'High', 'VeryHigh'])\n",
    "\n",
    "    # HDL Cholesterol (mg/dL)\n",
    "    data['hdl_level'] = pd.cut(data['hdl_cholesterol'],\n",
    "                            bins=[10, 40, 60, 100],\n",
    "                            labels=['Low', 'Normal', 'High'])\n",
    "\n",
    "    # LDL Cholesterol (mg/dL)\n",
    "    data['ldl_level'] = pd.cut(data['ldl_cholesterol'],\n",
    "                            bins=[30, 100, 130, 160, 190, 300],\n",
    "                            labels=['Optimal', 'NearOptimal', 'Borderline', 'High', 'VeryHigh'])\n",
    "\n",
    "    # Triglycerides (mg/dL)\n",
    "    data['triglycerides_level'] = pd.cut(data['triglycerides'],\n",
    "                                        bins=[20, 150, 200, 500, 1000],\n",
    "                                        labels=['Normal', 'Borderline', 'High', 'VeryHigh'])\n",
    "\n",
    "    # --- Glucose Metabolism ---\n",
    "\n",
    "    # Fasting Glucose (mg/dL)\n",
    "    data['glucose_fasting'] = pd.cut(data['glucose_fasting'],\n",
    "                                    bins=[50, 100, 126, 200, 400],\n",
    "                                    labels=['Normal', 'Prediabetic', 'Diabetic', 'Severe'])\n",
    "\n",
    "    # Postprandial Glucose (mg/dL)\n",
    "    data['glucose_postprandial'] = pd.cut(data['glucose_postprandial'],\n",
    "                                        bins=[50, 140, 200, 400],\n",
    "                                        labels=['Normal', 'Prediabetic', 'Diabetic'])\n",
    "\n",
    "    # Insulin (μU/mL)\n",
    "    data['insulin_level'] = pd.cut(data['insulin_level'],\n",
    "                                bins=[1, 5, 20, 40, 100],\n",
    "                                labels=['Low', 'Normal', 'Elevated', 'Severe'])\n",
    "\n",
    "    # HbA1c (%)\n",
    "    data['hba1c_level'] = pd.cut(data['hba1c'],\n",
    "                                bins=[3.5, 5.7, 6.5, 8, 10, 14],\n",
    "                                labels=['Normal', 'Prediabetic', 'Diabetic', 'PoorControl', 'Severe'])\n",
    "\n",
    "    # --- Risk & Stage ---\n",
    "\n",
    "    # Diabetes Risk Score (0–100)\n",
    "    data['diabetes_risk_score'] = pd.cut(data['diabetes_risk_score'],\n",
    "                                        bins=[0, 25, 50, 75, 90, 100],\n",
    "                                        labels=['Low', 'Mild', 'Moderate', 'High', 'Severe'])\n",
    "\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    categorical_features = [\n",
    "        'gender', 'ethnicity', 'education_level', 'income_level', \n",
    "        'employment_status', 'smoking_status',\n",
    "        'age_group', 'alcohol_group', 'activity_group', 'diet_group',\n",
    "        'sleep_group', 'family_history_diabetes', 'hypertension_history',\n",
    "        'cardiovascular_history', 'bmi_group', 'waist_ratio',\n",
    "        'systolic_bp', 'diastolic_bp', 'heart_rate', 'cholesterol_level',\n",
    "        'hdl_level', 'ldl_level', 'triglycerides_level', 'glucose_fasting',\n",
    "        'glucose_postprandial', 'insulin_level', 'hba1c_level', 'diabetes_risk_score',\n",
    "        'diabetes_stage', 'diagnosed_diabetes'\n",
    "    ]\n",
    "\n",
    "    processed_data = pd.get_dummies(data[categorical_features])\n",
    "\n",
    "    # Convert to boolean\n",
    "    finalized_data = processed_data.astype(bool)\n",
    "\n",
    "    print(finalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032c2873",
   "metadata": {},
   "source": [
    "## Mine Frequent Itemsets with Apriori & Associative Rule Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c18f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Mine Frequent Itemsets with Apriori\n",
    "frequent_itemsets = apriori(df, min_support =0.25, use_colnames =True)\n",
    "print(frequent_itemsets)\n",
    "\n",
    "# Generate Associative Rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.3)\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f38459",
   "metadata": {},
   "source": [
    "## Calculate Rule Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb3e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def compute_rule_weights(rules):\n",
    "    # Calculate weights for each rule based on support and confidence\n",
    "    weights = []\n",
    "    for _, row in rules.iterrows():\n",
    "        weight = row['confidence'] * math.log(1 + row['consequent support'])\n",
    "        weights.append(weight)\n",
    "    rules['weight'] = weights\n",
    "    return rules\n",
    "\n",
    "# Calculate Rule Weights\n",
    "weighted_rules = compute_rule_weights(rules)\n",
    "print(weighted_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ec76e",
   "metadata": {},
   "source": [
    "## Find Rules Applicable to Match Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39055025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rules_to_record(record, rules):\n",
    "    applicable_rules = []\n",
    "    for _, rule in rules.iterrows():\n",
    "        antecedents = set(rule['antecedents'])\n",
    "        if antecedents.issubset(set(record[record == True].index)):\n",
    "            applicable_rules.append(rule)\n",
    "    return applicable_rules\n",
    "\n",
    "# Find Rules Applicable to Record\n",
    "for index, row in df.iterrows():\n",
    "    applicable_rules = apply_rules_to_record(row, weighted_rules)\n",
    "    print(f\"Record {index} applicable rules:\")\n",
    "    total_weight = 0\n",
    "    for rule in applicable_rules:\n",
    "        total_weight += rule['weight']\n",
    "    df['Total Rule Weight'] = total_weight\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d583fe",
   "metadata": {},
   "source": [
    "## Apply Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3675557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_weights(record, applicable_rules):\n",
    "    total_weight = record['Total Rule Weight']\n",
    "    normalized_r_weight = []\n",
    "    for rule in applicable_rules:\n",
    "        normalized_weight = rule['weight'] / total_weight if total_weight > 0 else 0\n",
    "        normalized_r_weight.append((normalized_weight, rule))\n",
    "    return normalized_r_weight\n",
    "\n",
    "# Apply Normalization\n",
    "for index, row in df.iterrows():\n",
    "    applicable_rules = apply_rules_to_record(row, weighted_rules)\n",
    "    normalized_weights = normalize_weights(row, applicable_rules)\n",
    "    print(f\"Record {index} normalized rule weights:\")\n",
    "    for norm_weight, rule in normalized_weights:\n",
    "        print(f\"Rule: {rule['antecedents']} -> {rule['consequents']}, Normalized Weight: {norm_weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a42349",
   "metadata": {},
   "source": [
    "## Calculate Risk Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd3d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_risk_score(applicable_rules, normalized_weights):\n",
    "    \"\"\"\n",
    "    Compute a rule-based risk score for diabetes.\n",
    "    Score = sum(normalized_weight * rule_confidence for all applicable rules)\n",
    "    \"\"\"\n",
    "    if not applicable_rules:\n",
    "        return 0.0\n",
    "    score = 0.0\n",
    "    for (rule_id, weight), rule in zip(normalized_weights, applicable_rules):\n",
    "        score += weight * rule[\"confidence\"]\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3340c098",
   "metadata": {},
   "source": [
    "## Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a4dd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
